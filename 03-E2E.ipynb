{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006dbbc6-0b93-4006-bf95-d577ac76f2fa",
   "metadata": {},
   "source": [
    "# Narrow end-to-end story (but where are the 'ends' these days?)\n",
    "\n",
    "Here we'll take a look at\n",
    "* loading and reshaping data\n",
    "* training a model\n",
    "* serving\n",
    "\n",
    "... using a combination of tools.\n",
    "\n",
    "__The focus/goal is to share the *flavor* of the APIs and systems, not to go focus on solving specific problems__\n",
    "\n",
    ">We won't cover (but are certainly not neglecting the importance of) upstream activities like data acquisistion, discovery, and catalog integration...\n",
    "parallel work like experiment tracking, recording dataset provenance and features, archiving artifacts... or key downstream activities like monitoring models in production, drift or bias detection, rollout/rollback of new model versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3ed37-cef0-4b1a-9b1a-24c78025d9b6",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "For many use cases, the initial access of the data might be via Spark (or, e.g., Trino) in order to locate tables in __Nessie__ (https://projectnessie.org/) or a __Hive Metastore__ and to assemble/extract with (potentially complex) SQL.\n",
    "\n",
    "In this example, we'll assume we already know the locations of our data and we'll use Dask to access it.\n",
    "\n",
    "*By design, we are not going to create a Dask distributed cluster -- we'll use Dask to define some tasks but Ray to run them. If this is confusing, we'll have you covered in a couple of minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7220f692-a40f-48fb-9842-1c09a2aa3701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as ddf\n",
    "\n",
    "df = ddf.read_csv('data/diamonds.csv', dtype={'table':'float64'})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66677b-2784-4dbc-a766-2e04e5cd9ace",
   "metadata": {},
   "source": [
    "We can extend the Dask dataframe graph with some common data prep operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00217add-66ab-476e-bf80-c9dbd85a2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.categorize()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68987073-7793-43d8-9664-dbd1f7f7c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = ddf.get_dummies(df2)\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82163ba0-9a83-460a-9db4-d1f72b197e24",
   "metadata": {},
   "source": [
    "Ray can schedule (compute) the operations from a Dask task graph. In fact, Ray Data can integrate with lots of other data sources: https://docs.ray.io/en/latest/data/dataset.html#supported-input-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4c4d8-f3bf-4214-b5bc-ae1d6588c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6dbbe4-dc8b-413a-b934-be6b130ebc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.from_dask(df3)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075e769-23a0-4211-8cd5-ed61ba7528ab",
   "metadata": {},
   "source": [
    "We can do *some* data manipulation with Ray Data datasets.\n",
    "\n",
    "Today, Ray Data is envisioned as \"last-mile preprocessing\" along with assisting tasks that are specific to paralellism (e.g., repartition) or which require special handling in the parallel case (e.g., train/test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac2212-eb95-45c9-9f3d-c2a1c064e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.drop_columns('Unnamed: 0').repartition(2)\n",
    "\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd3b67-7308-42ba-9145-2ea905a5248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80865925-f2e4-4e05-ac67-3f660621a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = ds1.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c40554-ee63-47ef-8751-3bb7da882079",
   "metadata": {},
   "source": [
    "We can use the `Trainer` pattern (https://docs.ray.io/en/latest/train/train.html#intro-to-ray-train) -- here with XGBoost, but similarly for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612a9fa-937a-4122-b6f5-a3ba20bc43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scale = ScalingConfig(num_workers=2, use_gpu=False)\n",
    "\n",
    "trainer = XGBoostTrainer(scaling_config=scale,\n",
    "    label_column=\"price\",\n",
    "    num_boost_round=20,\n",
    "    params={ \"objective\": \"reg:squarederror\", \"eval_metric\": [\"rmse\", \"error\"], },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8e918-0c02-4786-a91f-dc6910bd47b7",
   "metadata": {},
   "source": [
    "If we had more time and wanted more accuracy, this would be a great point to try out __Ray Tune__ and get the best hyperparams we can: https://docs.ray.io/en/latest/tune/index.html\n",
    "\n",
    "Instead, we'll move toward serving this model via a low-latency request-response prediction service with __Ray Serve__.\n",
    "\n",
    "Before creating our service, let's make sure everything's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693b380-25ef-422e-b63a-d1baea692b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "predictor = XGBoostPredictor.from_checkpoint(result.checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaceeb5-ba8a-4b33-9d86-fbd0df810b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test = valid_dataset.drop_columns('price')\n",
    "\n",
    "smoke_test.to_pandas()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9eb26-4c26-47ac-bc79-84c0103a5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(smoke_test.to_pandas()[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea6fb84-5c9c-4e7c-b428-5343d80aaae0",
   "metadata": {},
   "source": [
    "Ok, now we'll create a service with Ray Serve to deploy our model.\n",
    "\n",
    "We'll serialize our last model checkpoint -- in production we could do something like this or use a model db or other mechanism to find the version we want to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0073e8-fa19-4526-88af-4f1ab690df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "checkpoint_serialized = cloudpickle.dumps(result.checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666deca2-0bd9-4b99-9ca2-78d760b6e676",
   "metadata": {},
   "source": [
    "At first, it might not be obvious why (or even whether) we want a system as complex as Ray for serving models.\n",
    "\n",
    "In this demo case, we could probably solve the problem other ways. But when we have multiple services, ensembling of models, conditional flow, autoscaling and heterogeneous hardware ... we'll be glad to have a tool designed for just such challenges.\n",
    "\n",
    "https://docs.ray.io/en/latest/serve/scaling-and-resource-allocation.html#autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82340e3-eb69-48fc-b7a3-72d10deab217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from starlette.requests import Request\n",
    "from typing import Dict\n",
    "from ray import serve\n",
    "\n",
    "@serve.deployment(route_prefix=\"/\", num_replicas=2)\n",
    "class DiamondPricerDeployment:\n",
    "    def __init__(self, checkpoint:bytes):\n",
    "        self._model = XGBoostPredictor.from_checkpoint(cloudpickle.loads(checkpoint))\n",
    "\n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        data = await request.json()\n",
    "        return { \"result\" : self._model.predict(pd.read_json(data)).predictions[0] }\n",
    "\n",
    "serve.run(DiamondPricerDeployment.bind(checkpoint=checkpoint_serialized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1d3ab-04fa-468f-8ec6-c0d5ce8d2e07",
   "metadata": {},
   "source": [
    "Ok... let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a690bc6-2c79-4d89-a170-d049f35278a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = smoke_test.to_pandas()[:1].copy(True)\n",
    "sample_row.carat = 0.8\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0590ed7-278d-4930-ad8b-d4fb7727596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.post(\"http://localhost:8000/\", json = sample_row.to_json()).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d680d-6358-41c1-aff5-a5365d70d9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
