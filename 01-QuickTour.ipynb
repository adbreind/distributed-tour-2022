{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e622c6-1731-472e-9fd2-006b40fb387f",
   "metadata": {},
   "source": [
    "# Three frameworks in 14 minutes (more or less)\n",
    "\n",
    "Background: these comparisons are not a \"horserace\" -- i.e., these tools are not directly comparable in functionality, so it's not a ranking, and none of them is inherently much faster than the others.\n",
    "\n",
    "The goal is to get a sense for which tools are best for which tasks so we can use one or all of them to build the systems we need to build.\n",
    "\n",
    "(chronologically...)\n",
    "\n",
    "## Apache Spark\n",
    "\n",
    "Probably the most famous of these tools, Spark...\n",
    "* was created around 2009 at UC Berkeley as a successor/companion to Hadoop big data tools\n",
    "  * motivation was leveraging memory and network (vs. storage)\n",
    "* was fast but not easily usable in the earliest iterations (-2015)\n",
    "* embraced SQL and dataframe patterns for users (plus better internals for performance) beginning in 2015 \n",
    "  * enjoys extensive success as a \"unified platform\" (data, ML, streaming, SQL)\n",
    "* leverages mainly Scala, with some other language wrappers (most notably Python and SQL)\n",
    "* is Apache Licensed and a top-level Apache Foundation project, with most core contributors at Databricks\n",
    "* works well if you have a good mental model of how it works and how to tune/troubleshoot; difficult and/or underperforming otherwise\n",
    "* hard-coded data-parallel scheduler pattern\n",
    "\n",
    "__* Strongest for: SQL, next-gen table formats [Delta Lake, Hudi, Iceberg], ETL, featurization from data lake[house], streaming, docs__\n",
    "\n",
    "__* Weakest for: integration with custom code, (in)flexible machine learning, \"grokking\" for tuning/troubleshooting__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2ea31-8512-4f3b-9df7-309d97c8a7b6",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "Popular among scientists and gaining general interest, Dask...\n",
    "* was created in 2014 as a pure-Python scheduler for multiprocessing\n",
    "  * motivation was to extend SciPy/PyData to arbitrarily large datasets\n",
    "  * and \"invent nothing\" (meaning leverage existing community code/libraries)\n",
    "* expanded to include Array, Dataframe, Bag (functional) collections, some ML as well as core scheduling primitives for Python functions\n",
    "* supports external integrations, e.g., it is used implicitly by multi-dimensional array library XArray\n",
    "* is pure Python (although many related Python libraries delegate to native code or accelerated code, e.g., NumPy, CuPy)\n",
    "* is BSD-3-Clause licensed (permissive)\n",
    "* has a core group of contributors with natural sciences research backgrounds\n",
    "  * functions largely on the \"classic volunteer OSS\" model with no corporate control\n",
    "* is fairly easy to get started with, especially if you know some Python and PyData tools\n",
    "\n",
    "__* Strongest for: array computation, scaling custom Python code, realtime visibility into processing (dashboards), \"grokking\" execution__\n",
    "\n",
    "__* Weakest for: tabular data access, \"off-the-shelf\" scalable machine learning, large-scale (data parallel) shuffle, docs__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294237c6-ed95-432b-b79d-0b618cfaf647",
   "metadata": {},
   "source": [
    "## Ray\n",
    "\n",
    "The newest of these frameworks and most rapidly evolving, Ray...\n",
    "\n",
    "* began in 2016-2017 at UC Berkeley featuring\n",
    "  * flexible, arbitrarily scalable abstraction over general function graphs and actors (think roughly \"distributed OO\")\n",
    "  * distributed scheduler\n",
    "* refactored in 2020-present as a layered platform with\n",
    "  * \"product-style/off-the-shelf\" solutions to common problems (data movement, scalable ML training including RL, tuning, deployment, and more)\n",
    "  * integrations to popular external components (e.g., Huggingface)\n",
    "  * a core layer for scaling custom code designs or building high-level components\n",
    "* is mostly Python with a Python API; some code components in C++ and \"plug points\" for alternative language bindings (e.g., Java)\n",
    "* is Apache licensed, but part of the Linux Foundation; leadership and core contributions from Anyscale\n",
    "* presents simple/effective high-level interfaces for common problems\n",
    "  * at the lower levels it's a but simpler than Spark but a bit more complex than Dask (YMMV)\n",
    "  \n",
    "__* Strongest for: scaling with minimal work: ML, DL, RL, tuning, deployment, etc.; API design focused on users; easy integration, docs__\n",
    "\n",
    "__* Weakest for: stable patterns/APIs due to fast evolution, updated design; early days for new APIs (e.g., AIR)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560fbbc-e6fe-4cfb-aaac-47019198b466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
