{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4201fa05-ec56-4cba-9ef4-e300ffd65beb",
   "metadata": {},
   "source": [
    "# Three frameworks in 59 minutes (more or less)\n",
    "\n",
    "Let's take a deeper dive into each of these tools and get into some code and architecture.\n",
    "\n",
    "For time reasons, and because we are interested in demoing where these tools work well, we'll just look at a few bits of key use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3b36a-b93b-4b1a-b3b1-2dd99276b434",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "__Data access__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcc467-67d8-4032-9d14-1d379d411b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f1416-6e6d-42df-b26d-33c5cd523d66",
   "metadata": {},
   "source": [
    "We can create a Spark dataframe from SQL. A Spark dataframe is really a query, not a dataset ... so it's closer to a VIEW in the RDBMS world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d1307-42dd-4ef2-aea9-7ade14361def",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM parquet.`data/california`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b92a91-6eaf-40e5-940e-f5b8a1361af2",
   "metadata": {},
   "source": [
    "We need to explicitly tell Spark if we want to read or process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d80200-d89e-4bea-9e72-afe59ecef3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM parquet.`data/california`').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fb1c1-9352-4cae-98ab-e9dd35be9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT origin, AVG(delay) as delay FROM parquet.`data/california` GROUP BY origin HAVING count(1) > 500 ORDER BY delay DESC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6d45d-d141-4a57-8668-277045afb7dd",
   "metadata": {},
   "source": [
    "__Data manipulation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3596d-277e-4aaf-8bd2-283df5b43719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/diamonds.csv', inferSchema=True, header=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682da7c6-2a0a-4a0c-aacb-79a1de197016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28314f33-1313-42a4-9fce-1acf39091771",
   "metadata": {},
   "source": [
    "We can manipulate Spark dataframes with the classic PySpark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76524e9e-9630-45c1-8564-773a0ecab6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('_c0').withColumnRenamed('price', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b64ded-f935-4782-b655-213255ab712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "df.groupby(fn.ceil('carat')).mean('price').orderBy('ceil(carat)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c4ece-c968-47eb-b0ac-b29d8fb6c2c5",
   "metadata": {},
   "source": [
    "In recent versions of Spark, we can also use the Pandas API (although there are a number of caveats that come with this approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fdeda-227c-4696-822b-a690b3ae4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "df.pandas_api()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfccaa-b047-42af-8f8f-a0033ebcff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = df.pandas_api().drop(columns='_c0').rename(columns={'price':'label'})\n",
    "\n",
    "psdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c85ed3-abfa-4fec-830a-736a3860cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.get_dummies(psdf)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697d799-eee6-44cf-a529-cc0f1a9c63ef",
   "metadata": {},
   "source": [
    "__Architecture__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c23b12-0fec-4436-a7aa-87c9f90700d7",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/h621Rva.png\" width=\"600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b18b81-36b0-4ba1-bd6a-10849c0105d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dask\n",
    "\n",
    "\n",
    "__Cluster creation and dashboards__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394dde4-4aa8-43f6-8166-398cffef2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit='1GB')\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56621f2c-014e-4431-9a9a-342ea9fe2dc2",
   "metadata": {},
   "source": [
    "* Dashboard\n",
    "* Jupyterlab plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e4923-6c4d-4a94-8d61-e1c052bbc665",
   "metadata": {},
   "source": [
    "__Arrays__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8acbe5-ad41-49ff-9bfc-888ef781f11b",
   "metadata": {},
   "source": [
    "Dask Array is a virtual, lazy large array composed of chunks, each of which will be a NumPy array (in the default configuration) when loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35d1de-a158-4095-985c-136d57d1bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "arr = da.random.random((200, 200), chunks=(50, 40))\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403b360-21e4-46f9-9f07-89612eaa6f8a",
   "metadata": {},
   "source": [
    "Dask Array aims to implement most of the NumPy API, so we use that API for most operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4d0cf-dda2-488e-9e4f-0a738ea59660",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr @ arr.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242f889-2fdb-4fd4-ac26-bc594dcc54ed",
   "metadata": {},
   "source": [
    "Because the data structure is virtual, we need to tell Dask explicitly what we want to `.compute()` or write out (e.g., via `.to_zarr()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a058d-17a5-48ad-badf-7c8b622bc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(arr @ arr.T).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda2220-635e-4e74-84dc-d221a82fc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.linalg.svd((arr @ arr.T).rechunk(200, 20)) # returns (u,s,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f97777-c9a7-43a4-b630-50276798797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.linalg.svd((arr @ arr.T).rechunk(200, 20))[1].compute() # singular vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bf4e8-d7a8-48b2-88a4-fcbed887e7a9",
   "metadata": {},
   "source": [
    "__Architecture__\n",
    "\n",
    "<img src='images/dask.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a028d2b-0ec9-4502-a673-44d65a14d858",
   "metadata": {},
   "source": [
    "__Parallelizing Python__\n",
    "\n",
    "Dask has two different APIs for parallelizing Python code. Here's we'll look at `delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45cd56-24ff-4c94-97e1-e38f68237f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import numpy as np\n",
    "\n",
    "@delayed\n",
    "def get_data(i):\n",
    "    return np.array([i, i+1, i+2])\n",
    "\n",
    "get_data(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a6693-f72c-4af5-ad85-e1c27975b80d",
   "metadata": {},
   "source": [
    "A Delayed is a proxy object (it can \"handle\" most normal operations/messages and internally records them into a compute graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da8bf2e-4eb8-4fe4-9ff4-6eb697f9ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(7).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916c674-69a1-4845-8090-4a3b3874c100",
   "metadata": {},
   "source": [
    "In its role as root of a compute graph, we can also tell it to `.compute`, cache (`.persist()`), explain (`.dask` or `.visualize()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b81c6-97cd-440f-b64f-c6fd8e0cd505",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_numbers = get_data(7)\n",
    "\n",
    "some_more = get_data(100)\n",
    "\n",
    "total = np.sum(some_numbers) + np.sum(some_more)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29c227-fc85-4796-b046-44a6d1ef0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795924b-7c27-4074-9d9b-b725166cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee304e50-997e-4376-aa37-4492ee9d32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfbb78-6753-44ff-a2eb-cc5c11d86e0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f625e-db6f-4939-ad4c-b63dee25c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(num_cpus=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f84810-37c8-48df-8729-6b82a7734e40",
   "metadata": {},
   "source": [
    "__Data access__\n",
    "\n",
    "Ray accesses data from storage or from other systems via Ray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f41b3-4adc-4443-a465-8407144f171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ray.data.read_csv('data/breast_cancer.csv')\n",
    "\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874aef9-6fb9-4753-9d5f-d42723e64aa4",
   "metadata": {},
   "source": [
    "__Prep and model training__\n",
    "\n",
    "Ray Data is also capable of some data manipulation (\"last-mile data prep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706960de-bf96-44fa-b008-7383b1f87ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e76d4b9-b057-4c41-8f57-ba31f2b01e1e",
   "metadata": {},
   "source": [
    "Training is done through a standardized `Trainer` interface that allows for tree-based, deep-learning, or other distributed training use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea071cac-21c4-4b41-b9ee-2692e20f571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "scale = ScalingConfig(num_workers=2, use_gpu=False)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config = scale, label_column=\"target\", num_boost_round=20,\n",
    "    \n",
    "    params={ \"objective\": \"binary:logistic\", \"eval_metric\": [\"logloss\", \"error\"] }, # XGBoost params\n",
    "    \n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74785d88-9daf-4ba8-a92f-e76ee648f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546b17b-0c92-45d9-b021-304afa29463a",
   "metadata": {},
   "source": [
    "__Architecture__\n",
    "\n",
    "https://docs.ray.io/en/latest/cluster/key-concepts.html#key-concepts\n",
    "\n",
    "<img src='images/ray-cluster.svg' width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f6c58-2c01-4123-bf14-d787c26ffbe4",
   "metadata": {},
   "source": [
    "__Prediction__\n",
    "\n",
    "Batch prediction has a dedicated API (a separate API is used for fast/small prediction, which we'll see when we demo Ray Serve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd74af-0afb-4020-b768-9595eae76c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.xgboost import XGBoostPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor.from_checkpoint(result.checkpoint, XGBoostPredictor)\n",
    "\n",
    "demo_records = valid_dataset.drop_columns(['target'])\n",
    "\n",
    "batch_predictor.predict(demo_records).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6cffa-8d0b-4067-b271-93f2d4863cc6",
   "metadata": {},
   "source": [
    "__Reinforcement Learning__\n",
    "\n",
    "RL is a key use case with extensive design and support in Ray. It's outside our scope for today but definitely check it out.\n",
    "\n",
    "<video src='images/cpv1.mp4' controls='true' autoplay='true' loop='true' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd45d2-339f-4293-87fc-9fa2272e4832",
   "metadata": {},
   "source": [
    "See the latest examples at https://docs.ray.io/en/latest/rllib/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9192e23-feb2-4285-842a-e29ccd08c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f0cee-7398-4812-9884-9b10cb5e7e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
